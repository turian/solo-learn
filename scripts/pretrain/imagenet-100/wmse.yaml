name: "wmse-imagenet100"
method: "wmse"
backbone:
  name: "resnet18"
method_kwargs:
  proj_hidden_dim: 1024
  proj_output_dim: 64
  whitening_size: 128
data:
  dataset: imagenet100
  train_path: "datasets/imagenet100/train"
  val_path: "datasets/imagenet100/val"
  format: "dali"
  num_workers: 4
  augmentations:
    - rrc:
        enabled: True
        crop_min_scale: 0.2
        crop_max_scale: 1.0
      color_jitter:
        enabled: True
        brightness: 0.8
        contrast: 0.8
        saturation: 0.8
        hue: 0.2
        prob: 0.8
      grayscale:
        enabled: True
        prob: 0.2
      gaussian_blur:
        enabled: True
        prob: 0.2
      solarization:
        enabled: False
      equalization:
        enabled: False
      horizontal_flip:
        enabled: True
        prob: 0.5
      crop_size: 224
      num_crops: 2
    - rrc:
        enabled: True
        crop_min_scale: 0.2
        crop_max_scale: 1.0
      color_jitter:
        enabled: True
        brightness: 0.8
        contrast: 0.8
        saturation: 0.8
        hue: 0.2
        prob: 0.8
      grayscale:
        enabled: True
        prob: 0.2
      gaussian_blur:
        enabled: True
        prob: 0.2
      solarization:
        enabled: False
      equalization:
        enabled: False
      horizontal_flip:
        enabled: True
        prob: 0.5
      crop_size: 96
      num_crops: 6
optimizer:
  name: "adam"
  batch_size: 64
  lr: 2e-3
  classifier_lr: 3e-3
  weight_decay: 1e-6
scheduler:
  name: "warmup_cosine"
  warmup_start_lr: 0
checkpoint:
  enabled: True
  dir: "trained_models"
  frequency: 1
auto_resume:
  enabled: True
augmentations_cfg: "scripts/configs/defaults/augmentations/asymmetric/default.yaml"
wandb_cfg: "scripts/configs/defaults/wandb/private.yaml"
# overwrite PL stuff
max_epochs: 400
devices: [0, 1]
sync_batchnorm: True
accelerator: "gpu"
strategy: "ddp"
precision: 16